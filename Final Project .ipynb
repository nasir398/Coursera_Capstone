{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Major attractions nearby and Data Science jobs (in MA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date: 21 March 2019\n",
    "#### Author: Nasir Ahmad\n",
    "#### Email : nasir398@hotmail.com\n",
    "#### Publication: This article is purely based on personal research for non commercial usages. The services used by third party are credited without any intention of copy right violation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science jobs are available around the globe yet the hunt for job is not easy when you have too many places to choose from. In this article I will focus on the types of locations where Data Science jobs are available in Massachusetts US. \n",
    "There are two approaches to get any job:\n",
    "1.\tLook for a new job around your current location. \n",
    "2.\tSearch with keyword and then go through each job description. \n",
    "\n",
    "Both of above methods fail to include one basic question: What kind of neighborhood is the company located in? Everyone knows the high-tech jobs are available in Silicon Valley, Seattle, New York, and Boston and so on. But what if you are not a big fan of living in a populous city, what if you just want a peaceful country side to live and code for living! \n",
    "This article answers the following question:\n",
    "\n",
    "**“What are the major attractions nearby locations where Data Science jobs are being offered in Massachusetts?”**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Foursquare is a platform which provides information about places in a given neighborhood. In this article Foursquare‘s listing is used to get popular sites in a given area.  It provides developers with a good API which provides result in the form of JSON file, which developer can comprehend as per requirements. \n",
    "\n",
    "https://foursquare.com/city-guide\n",
    "\n",
    "2. Adzuna (A) is a website which provides jobs listing for any given location in addition to other useful services related to jobs. This article uses Adzuna developer API for information related to Data Science jobs in Massachusetts. \n",
    "\n",
    "https://www.adzuna.com/\n",
    "\n",
    "3. Massachusetts State is divided into 14 counties as shown in below image. \n",
    "\n",
    "https://upload.wikimedia.org/wikipedia/commons/b/b6/Massachusetts-counties-map.gif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lets start by importing libraries which will be used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adzuna API for developer has detailed documentation, which provides the JSON file specialized for a geo location. \n",
    "I recommend to use this link : https://api.adzuna.com/static/swagger-ui/index.html#!/adzuna\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a developer account which is free, providing user with app_id and app_key, prerequisite for API calling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set paramaters for API call\n",
    "app_id ='51c4103a' # assigned by Adzuna\n",
    "app_key='6bf4141ed2bb578e5c972eeb1a68e5b7' # Assigned by Adzuna\n",
    "search_word='Data Science'#Keyword for which search is going to be performed \n",
    "search_location='Massachusetts' # location where the job search will be performed \n",
    "\n",
    "url ='https://api.adzuna.com:443/v1/api/jobs/us/geodata?app_id={}&app_key={}&what={}&where={}'.format(app_id,app_key,search_word,search_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use requests to call the url and get the JSON file in return. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = requests.get(url).json()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON file is good for communication but in notebook we can accomplish much more with dataframes. Up next we normalize the JSON file into dataframe which looks like below :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_ma = results['locations']  \n",
    "data_jobs_ma = json_normalize(jobs_ma) # flatten JSON\n",
    "data_jobs_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our cause the data required for research is number of jobs in each County so we clean the data and get our required data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns and clean data\n",
    "filtered_columns = ['count', 'location.area', 'location.display_name']\n",
    "data_jobs_ma =data_jobs_ma.loc[:, filtered_columns]\n",
    "data_jobs_ma.columns = [col.split(\".\")[-1] for col in data_jobs_ma.columns]\n",
    "data_jobs_ma.rename({'count':'job_count','display_name':'Neighborhood'}, axis=1, inplace=True)\n",
    "#data_jobs_ma =data_jobs_ma.drop(labels='area',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_jobs_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the Data in form of Bar graph & see how it feels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dataframe which is graph friendly \n",
    "jobs_ma_display = data_jobs_ma\n",
    "#jobs_ma_display.Neighborhood=[col.split(\",\")[0] for col in jobs_ma_display.Neighborhood]\n",
    "jobs_ma_display['Neighborhood'] = jobs_ma_display['Neighborhood'].str.replace(', Massachusetts','')\n",
    "jobs_ma_display = jobs_ma_display.set_index(jobs_ma_display.Neighborhood)\n",
    "jobs_ma_display =jobs_ma_display.drop(labels='Neighborhood',axis=1)\n",
    "\n",
    "#assign the type of plot as Bar\n",
    "jobs_ma_display.plot(kind='bar',figsize=(15, 9))\n",
    "\n",
    "plt.xlabel('MA County')\n",
    "plt.ylabel('Number of Data Scienc Jobs')\n",
    "\n",
    "plt.title('Data Science jobs in Massachusetts', y=1) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly most of the Data Science jobs are in Middlesex and Suffolk counties. Now let’s break the details of job count in counties other than these two by plotting a pie chart.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The default colors and boundries are not much pretty to see so i perfer my own ones\n",
    "colors_list = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'orange', 'pink','brown','black','red','blue','silver']\n",
    "explode_list = [0,  0,0,0.03,0.04,0.05,0.08, 0.22, 0.44] # ratio for each county with which to offset each wedge.\n",
    "\n",
    "#Choose the jobs for each county less than 100. This criteria makes sure the major counties are excluded from our data. \n",
    "jobs_ma_display = jobs_ma_display[jobs_ma_display['job_count'] <100]\n",
    "\n",
    "\n",
    "jobs_ma_display['job_count'].plot(kind='pie',\n",
    "                            figsize=(20, 9),\n",
    "                            autopct='%1.1f%%', \n",
    "                            startangle=90,    \n",
    "                            shadow=True,       \n",
    "                            labels=None,         # turn off labels on pie chart\n",
    "                            pctdistance=1.2,    # the ratio between the center of each pie slice and the start of the text generated by autopct \n",
    "                            colors=colors_list,  # add custom colors\n",
    "                            explode=explode_list # 'explode' lowest counties\n",
    "                            )\n",
    "\n",
    "\n",
    "plt.title(' Data Science jobs in Massachusetts Except Middlesex & Suffolk county', y=1.12) \n",
    "plt.axis('equal') \n",
    "\n",
    "# add legend\n",
    "plt.legend(labels=jobs_ma_display['job_count'].index, loc='upper left') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Worcester and Norfolk take the lead now by conquering major portion of the data science jobs outside of Middlesex and Suffolk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings us to the end of Adunza section. \n",
    "Next we will explore Foursquare API for places in Massachusetts state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foursquare API heavily relies on geographical coordinates, although address can be used but to be on the safe side we use latitude and longitude of each county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to return the  latitude and longitude\n",
    "def get_log_lat(address):\n",
    "    try:\n",
    "        \n",
    "        geolocator = Nominatim(user_agent=\"ma_explorer\")\n",
    "        location = geolocator.geocode(address)\n",
    "        latitude = location.latitude\n",
    "        longitude = location.longitude\n",
    "    except :\n",
    "        latitude=-1\n",
    "        longitude=-1\n",
    "    return  latitude , longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets loop thorough the county of Massachusetts to add coordinates \n",
    "data_jobs_ma['latitude']=0\n",
    "data_jobs_ma['longitude']=0\n",
    "i=0\n",
    "for row in data_jobs_ma['area']:\n",
    "    print(row)\n",
    "    data_jobs_ma['latitude'].iloc[i],data_jobs_ma['longitude'].iloc[i]=get_log_lat(row)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm the changes explore the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_ma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Massachusetts using Foursquare API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foursquare API provides data about sites in a given nearby place. Below are the important information required for calling API: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = 'BKONW20YSKTSNRHM2ECIEPYFYA14SSA4QELHWVVP25N5SJLZ' # your Foursquare ID\n",
    "CLIENT_SECRET = '1VZQZHXLZ3TT1CNSJ2O113HROIHMIPU24KF3MDLE1J4OSSJI' # your Foursquare Secret\n",
    "VERSION = '20180604' \n",
    "radius= 22000 # the approx radius of smallest county\n",
    "LIMIT = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a funtion to invoke the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=22000):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        \n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood', \n",
    "                  'Neighborhood Latitude', \n",
    "                  'Neighborhood Longitude', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the above code for Neighborhood of MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_venues = getNearbyVenues(names=data_jobs_ma['Neighborhood'],\n",
    "                                   latitudes=data_jobs_ma['latitude'],\n",
    "                                   longitudes=data_jobs_ma['longitude']\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ma_venues.shape)\n",
    "ma_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the count of each Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_venues.groupby('Neighborhood').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "ma_onehot = pd.get_dummies(ma_venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add neighborhood column back to dataframe\n",
    "ma_onehot['Neighborhood'] = ma_venues['Neighborhood'] \n",
    "\n",
    "# move neighborhood column to the first column\n",
    "fixed_columns = [ma_onehot.columns[-1]] + list(ma_onehot.columns[:-1])\n",
    "ma_onehot = ma_onehot[fixed_columns]\n",
    "\n",
    "ma_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see the stats \n",
    "\n",
    "ma_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, let's group rows by neighborhood and by taking the mean of the frequency of occurrence of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_grouped = ma_onehot.groupby('Neighborhood').mean().reset_index()\n",
    "ma_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_grouped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Let's print each neighborhood along with the top 5 most common venues\n",
    "num_top_venues = 8\n",
    "\n",
    "for hood in ma_grouped['Neighborhood']:\n",
    "    print(\"----\"+hood+\"----\")\n",
    "    temp = ma_grouped[ma_grouped['Neighborhood'] == hood].T.reset_index()\n",
    "    temp.columns = ['venue','freq']\n",
    "    temp = temp.iloc[1:]\n",
    "    temp['freq'] = temp['freq'].astype(float)\n",
    "    temp = temp.round({'freq': 2})\n",
    "    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Let's put that into a pandas dataframe\n",
    "##First, let's write a function to sort the venues in descending order.\n",
    "\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create the new dataframe and display the top 10 venues for each neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_venues = 10\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['Neighborhood']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "neighborhoods_venues_sorted = pd.DataFrame(columns=columns)\n",
    "neighborhoods_venues_sorted['Neighborhood'] = ma_grouped['Neighborhood']\n",
    "\n",
    "for ind in np.arange(ma_grouped.shape[0]):\n",
    "    neighborhoods_venues_sorted.iloc[ind, 1:] = return_most_common_venues(ma_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "neighborhoods_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_grouped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology : K- Means Cluster County"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the objective of our article is to find out possible location's so let us run *k*-means to cluster the neighborhood into 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 3\n",
    "\n",
    "ma_grouped_clustering = ma_grouped.drop('Neighborhood', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(ma_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new dataframe that includes the cluster as well as the top 10 venues for each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_jobs_ma.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "neighborhoods_venues_sorted.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "ma_merged = data_jobs_ma\n",
    "\n",
    "# merge  with toronto_data to add latitude/longitude for each neighborhood\n",
    "ma_merged = ma_merged.join(neighborhoods_venues_sorted.set_index('Neighborhood'), on='Neighborhood')\n",
    "ma_merged =ma_merged.drop(labels='area',axis=1)\n",
    "\n",
    "ma_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize the resulting clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create map\n",
    "latitude=42.40 \n",
    "longitude=-71.38\n",
    "map_clusters = folium.Map(location=[latitude, longitude], zoom_start=8)\n",
    "folium.TileLayer('cartodbpositron').add_to(map_clusters)\n",
    "\n",
    "# set color scheme for the clusters\n",
    "x = np.arange(kclusters)\n",
    "ys = [i + x + (i*x)**2 for i in range(kclusters)]\n",
    "colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "rainbow = [colors.rgb2hex(i) for i in colors_array]\n",
    "\n",
    "# add markers to the map\n",
    "markers_colors = []\n",
    "for lat, lon, poi, cluster,job_count in zip(ma_merged['latitude'], ma_merged['longitude'], ma_merged['Neighborhood'], ma_merged['Cluster Labels'],ma_merged['job_count']):\n",
    "    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lon],\n",
    "        radius=15+np.log(job_count),\n",
    "        popup=label,\n",
    "        color=rainbow[cluster-1],\n",
    "        fill=True,\n",
    "        weight = 5,\n",
    "        fill_color=rainbow[cluster-1],\n",
    "        fill_opacity=0.7).add_to(map_clusters)\n",
    "       \n",
    "map_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
